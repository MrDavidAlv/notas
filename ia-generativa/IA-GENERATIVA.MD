
<div align="center">

# RESUMEN GENERAL SOBRE IA GENERATIVA 

<img src="./imagenes/portada.png" width="20%">

</div>

> üìñ Basado en el libro *IA Generativa en AWS* de Marbombo, autores: Chris, Antje y Shelbee.

## 1. CASOS DE USO DE LA IA GENERATIVA Y CICLO DE VIDA DEL PROYECTO

### Servicios AWS para IA 
- Bedrock
- SageMarker
- CodeWhispperer
- Trainium
- Inferentia

### Casos de uso
- Resumen texto
- Reescritura
- Extracci√≥n de informaci√≥n
- Respuesta a preguntas
- Detecci√≥n de contenido t√≥xico o da√±ino
- Clasificaci√≥n y moderaci√≥n de contenido
- Interfaz conversacional
- Traducci√≥n 
- Generaci√≥n de c√≥digo fuente
- Razonamiento sobre un bosquejo dibujado a mano
- Enmascarar informaci√≥n personal identificable(PII)
- Marketing personalizado y anuncios

### Modelos b√°sicos y centros de modelos
- Modelos de redes neuronales grandes y complejos (con cantidades enormes de par√°metros).
    > üìå **NOTA:**  Los par√°metros aprenden en una etapa de aprendizaje llamada *preformaci√≥n*, donde se utilizan grandes cantidades de datos distribuidos en CPU y GPU con lo cual al final estos pueden representar entidades complejas como el lenguaje humano, im√°genes, v√≠deos y fragmentos de audio. 

### Fuentes de modelos de IA
- Hugging Face Model Hub
- Pythorch Hub
- Amazoz SageMaker JumpStart

### Ciclo de vida del proyecto de IA generativa
No existe un ciclo de vida definitivo para un proyecto de IA generativa, pero esto podr√≠a concentrar las etapas m√°s importantes de un ciclo de proyecto IA.

<div align="center">
    <img src="./imagenes/ciclo-vida-proyecto-IA.png" width="70%">
</div>




- **Identifique caso de uso**
    Aqu√≠ se define el caso de uso, alcance y tarea espec√≠fica con el fin de tener claro los objetivos, comprender el potencial y las limitaciones. 
- **Experimente y seleccione** 
    Se utilizan t√©cnicas de *ingenier√≠a de indicaciones* y *aprendizaje de contexto*. Inicie con un modelo b√°sico existente y ajuste seg√∫n la necesidad de la aplicaci√≥n. Eval√∫e tama√±o y recursos, probando distintas opciones en entornos como Amazon SageMaker, Hugging Face u otros para iterar r√°pido antes de escalar.
- **Adapte alinee y mejore** 
    Personalice el modelo al dominio espec√≠fico y caso de uso, afinando datos y par√°metros. Use t√©cnicas como RLHF para alinearlo con valores y preferencias humanas, y complem√©ntelo con fuentes externas o APIs.
- **Eval√∫e** 
    Defina m√©tricas claras para medir mejoras y alineaci√≥n con metas empresariales. La evaluaci√≥n continua durante la adaptaci√≥n asegura que el modelo evolucione en la direcci√≥n correcta.
- **Instale e integre** 
    Implemente el modelo optimizando la inferencia y reduciendo latencia para mejorar la experiencia del usuario. Utilice instancias de AWS optimizadas para cargas generativas y puntos de conexi√≥n flexibles.
- **Monitoree**  
    Configure m√©tricas y alertas con servicios como Amazon CloudWatch y CloudTrail para supervisar rendimiento y seguridad. Mantenga un control constante para ajustar y optimizar en producci√≥n.

![Ciclo de vida](/imagenes/ciclo-vida.png)

### ¬øPor qu√© IA generativa en la nube?

- **Mayor flexibilidad y elecci√≥n**
    La nube ofrece la posibilidad de acceder a una amplia gama de servicios, modelos y herramientas para ajustarse a diferentes necesidades y casos de uso. Permite elegir el modelo adecuado para un proyecto y cambiarlo cuando sea necesario, aprovechando de forma continua las mejoras y nuevos modelos que aparecen.
- **Funciones de seguridad y gesti√≥n de nivel empresarial**
    Los proveedores de nube suelen integrar medidas avanzadas de seguridad y gesti√≥n para proteger datos, redes y accesos. Esto incluye cifrado, aislamiento de entornos, autenticaci√≥n, autorizaci√≥n y detecci√≥n de amenazas, lo que resulta esencial en sectores con altos requerimientos regulatorios. 

- **Capacidades generativas de √∫ltima generaci√≥n**
    Las plataformas en la nube facilitan el uso de modelos generativos de c√≥digo abierto y propietario, as√≠ como herramientas para entrenamiento, ajuste fino e implementaci√≥n a gran escala. Adem√°s, los proveedores invierten en infraestructura optimizada para IA, reduciendo los tiempos y costos de desarrollo.
- **Gastos operativos reducidos**
    Gracias a la infraestructura gestionada, la nube permite usar IA generativa sin tener que invertir en hardware propio, reduciendo gastos y simplificando la administraci√≥n.
- **Un historial s√≥lido de innovaci√≥n continua**
    Las principales plataformas de nube mantienen un ritmo constante de innovaci√≥n en inteligencia artificial, lanzando mejoras y nuevos servicios que ayudan a implementar soluciones m√°s r√°pidas y eficientes.

### Creaci√≥n de aplicaciones de IA generativa

Una aplicaci√≥n de IA generativa incluye modelos generativos.

> üìå **NOTA:** Generalmente, los proveedores de la nube ofrecen herramientas que empaquetan estas arquitecturas, como lo es CodeWhisper de AWS.

**Diagrama APP general**

<div align="center">

<img src="./imagenes/AppIA.jpeg" width="70%">

</div>

**Diagrama APP ejemplo en AWS**
<div align="center">
 <img src="./imagenes/AppIAAWS.jpeg" width="70%">
</div>

---

## 2. INGENIER√çA DE INDICACIONES Y APRENDIZAJE EN CONTEXTO 
Incluye m√©todos de c√≥digo bajo para interactuar con modelos de IA generativa. *"Escribir indicaciones es tanto un arte como una ciencia"*.  
Aprendizaje en *contexto*: uso de pares de preguntas y respuestas.  
Ajuste de par√°metros generativos, como la temperatura y *top-k*, para controlar la creatividad del modelo.
Componentes l√©xicos(tokens) de texto.

### Indicaciones y respuestas
Generalmente, las indicaciones se dan en texto y se da una respuesta en texto. En modelos multimodales, devuelve otro tipo de respuestas: la respuesta puede contener texto, imagen, audio o video. El modelo aprender√° a optimizar las respuestas para dar el mejor resultado.   


### Componentes l√©xicos
Los modelos de IA generativa entienden las instrucciones en contexto a trav√©s de tokens, en los cuales las palabras se fragmentan y combinan de diferentes maneras, integrando diversos componentes l√©xicos.

<div align="center">
 <img src="./imagenes/token.png" width="70%">
</div>

> **Token üß©:** En el contexto de la inteligencia artificial y el procesamiento de lenguaje natural, un token es una unidad m√≠nima de texto que el modelo utiliza para procesar y generar lenguaje.
Puede ser una palabra completa, parte de una palabra, un signo de puntuaci√≥n o incluso un espacio, dependiendo del sistema de tokenizaci√≥n.
Los modelos transforman el texto en tokens para convertirlo en representaciones num√©ricas que pueden ser interpretadas y manipuladas mediante c√°lculos algebraicos y estad√≠sticos.

> üìå **Ventajas:**  Esta forma de estructurar el vocabulario permite que el modelo aprenda y comprenda el lenguaje humano durante la etapa de preentrenamiento. Adem√°s, facilita el c√°lculo de la respuesta, ya que esta es el resultado de operaciones algebraicas y estad√≠sticas.


### Ingenier√≠a de indicaciones 

Habilidad enfocada en comprender y aplicar modelos de IA generativa a tareas y casos de uso, con el objetivo de obtener el m√°ximo provecho de los modelos y sus aplicaciones.
Generalmente, se itera m√∫ltiples veces antes de obtener la respuesta final, ya que los modelos suelen generar grandes vol√∫menes de texto.
La mayor√≠a de los modelos de IA generativa han sido afinados con la ayuda de personas que etiquetan los resultados, en un proceso conocido como *aprendizaje por refuerzo*.

> **aprendizaje por refuerzo** üéØ En inteligencia artificial, el aprendizaje por refuerzo es un enfoque en el que un agente aprende a tomar decisiones mediante la interacci√≥n con un entorno, recibiendo recompensas o penalizaciones seg√∫n sus acciones.
En el caso de la IA generativa, este m√©todo se emplea para afinar modelos utilizando la retroalimentaci√≥n de personas que eval√∫an y etiquetan las salidas del modelo, ajustando as√≠ su comportamiento para alinearlo con objetivos, valores o preferencias humanas.

En una charla es importante indicar las entradas, por ejemplo, del usuario como `Usuario:`, y para las salidas del modelo utilizar `Asistente:`. Esto le da una estructura clara que facilita la interacci√≥n.

### Estructura de indicaciones

Para mejorar la calidad de la respuesta de un modelo de IA generativa, es fundamental optimizar la estructura de la indicaci√≥n.  
El modelo debe tener **clara la instrucci√≥n** y contar con **contexto suficiente** para interpretar correctamente la solicitud.

- **Instrucci√≥n:** Texto de entrada que describe de forma clara y precisa lo que se desea que el modelo realice.  
  Mientras m√°s espec√≠fica y detallada sea la instrucci√≥n, mejor ser√° la calidad de la respuesta.

- **Contexto:** Informaci√≥n adicional y detalles relevantes que ayudan al modelo a comprender mejor la solicitud.  
  Este puede incluir datos previos, ejemplos o referencias espec√≠ficas.

Adem√°s, se pueden incluir otros elementos para orientar la respuesta del modelo:

- **Tono:** Indica el estilo de comunicaci√≥n que debe utilizar el modelo (formal, informal, t√©cnico, amigable, persuasivo, etc.).

- **Restricciones:** L√≠mites o condiciones que debe respetar la respuesta, como extensi√≥n m√°xima, uso de cierto vocabulario o evitar determinados temas.

### Inferencia con cero golpes
Pido al asistente: *‚ÄúEscr√≠beme un haiku sobre la lluvia‚Äù*.  
No doy ejemplos, solo la instrucci√≥n, y espero la respuesta en el formato correcto.  

> üìå **Descripci√≥n:** El modelo responde √∫nicamente con lo aprendido en su entrenamiento, sin ejemplos adicionales.  
> Es √∫til cuando la tarea es sencilla o bien definida.  


### Inferencia con un golpe
Pido al asistente: *‚ÄúEscr√≠beme un haiku sobre la lluvia‚Äù*  
y le muestro un ejemplo de la estructura:  

> *‚ÄúViejo estanque,  
salta una rana al agua,  
ruido de agua.‚Äù*  

> üìå **Descripci√≥n:** El modelo recibe un ejemplo como referencia y lo usa para imitar la forma y el estilo esperados.  
> Es √∫til cuando se requiere un formato o tono espec√≠fico.  


### Tipos de inferencia con ejemplos

**Inferencia con un solo golpe**
Se agrega una instrucci√≥n con un ejemplo de referencia.

Indicaci√≥n:
Usuario: Responda utilizando el formato mostrado. ¬øQui√©n gan√≥ la serie mundial de b√©isbol en 2016?
Los Astros de Houston ganaron la serie mundial en 2022. Derrotaron a los Filis de Filadelfia.
¬øQui√©n gan√≥ la serie mundial de b√©isbol en 2016?

Respuesta: Los Cachorros de Chicago ganaron la serie mundial de 2016. Derrotaron a los Indios de Cleveland en siete partidos...

> üìå **NOTA:** Respuesta m√°s cercana a lo deseado, pero con demasiado detalle.

**Inferencia con pocos golpes**
Se proporcionan m√∫ltiples ejemplos para establecer un patr√≥n claro.

**Cuando el aprendizaje sale mal**
Si se proporcionan ejemplos incorrectos, el modelo aprende temporalmente informaci√≥n err√≥nea. Ejemplo: clasificar opiniones positivas como negativas y viceversa.

### Mejores pr√°cticas de aprendizaje en contexto

- **Comience con cero golpes**, luego pruebe uno o pocos golpes si es necesario
- **Proporcione ejemplos consistentes** que representen bien el conjunto de datos
- **L√≠mite: 5-6 ejemplos m√°ximo**. Si necesita m√°s, considere afinar el modelo
- **El aprendizaje en contexto var√≠a** seg√∫n el modelo utilizado

### Mejores pr√°cticas de ingenier√≠a de indicaciones

#### Estructura y claridad
- **Sea claro y conciso**: Evite ambig√ºedad. Si confunde a humanos, confundir√° al modelo
- **Ser creativo**: Indicaciones reflexivas generan respuestas innovadoras
- **Instrucci√≥n al final**: Para textos largos, mueva la instrucci√≥n al final antes del indicador de salida

#### Especificaciones t√©cnicas
- **Transmita el tema claramente**: Especifique qui√©n, qu√©, d√≥nde, cu√°ndo, por qu√©, c√≥mo
- **Directivas expl√≠citas**: "Resuma en una sola frase" vs descripciones vagas
- **Evite formulaciones negativas**: Use "m√°ximo 10 palabras" vs "no m√°s de 10 palabras"
- **Especifique tama√±o de respuesta**: Incluya l√≠mites de longitud espec√≠ficos

#### T√©cnicas avanzadas
**Cadena de pensamientos (CoT)**
Para problemas complejos, agregue "piense paso a paso":

Indicaci√≥n: ¬øQu√© veh√≠culo necesita m√°s pago inicial? (piense paso a paso)
Veh√≠culo A: $40,000 con 30% inicial
Veh√≠culo B: $50,000 con 20% inicial

Respuesta: Veh√≠culo A necesita m√°s pago inicial.
A: $40,000 √ó 30% = $12,000
B: $50,000 √ó 20% = $10,000
Por tanto, A requiere $2,000 m√°s.

**Manejo de incertidumbre**
Defina qu√© hacer si el modelo no sabe: "Si no est√° seguro, responda 'No s√©'"

**Restricciones de responsabilidad**
Para dominios especializados: "No tengo licencia m√©dica. Consulte un profesional"

**Etiquetas XML/HTML**
Use `<text>...</text>` para estructurar indicaciones complejas y `<respuesta>...</respuesta>` para formatear salidas.

### Par√°metros de configuraci√≥n de inferencia

<div align="center">
    <img src="./imagenes/parametros-configuracion-inferencia.jpeg" width="70%">
</div>

#### Par√°metros principales

**Max new tokens**
- Limita componentes l√©xicos generados
- Controla longitud, no previene alucinaciones

**Estrategias de muestreo**
- **Muestreo codificioso**: Siempre elige palabra m√°s probable (predeterminado)
- **Muestreo aleatorio**: Selecci√≥n ponderada aleatoria, m√°s natural

<div align="center">
    <img src="./imagenes/muestreo-codicioso-muestreo-aleatorio.jpeg" width="70%">
</div>

**Top-k**
Limita selecci√≥n a los k componentes m√°s probables. Si k=3, elige aleatoriamente entre las 3 opciones principales.

<div align="center">
    <img src="./imagenes/muestreo-aleatorio-top-K.jpeg" width="70%">
</div>

**Top-p**
Selecciona componentes cuyas probabilidades acumuladas no excedan p. Ejemplo: p=0.32 incluye componentes con probabilidades 0.30, 0.20, 0.10, 0.02.

<div align="center">
    <img src="./imagenes/ponderacion-probabilidad-aleatoria.jpeg" width="70%">
</div>

**Temperatura**
Controla aleatoriedad modificando distribuci√≥n de probabilidad:
- **Baja (<1)**: M√°s conservador, concentra probabilidades
- **Alta (>1)**: M√°s creativo, distribuye probabilidades uniformemente  
- **1**: Mantiene distribuci√≥n original del modelo

<div align="center">
    <img src="./imagenes/temperatura-comparacion.jpeg" width="70%">
</div>

> ‚ö†Ô∏è **Cuidado**: Temperatura muy baja = repeticiones. Temperatura muy alta = sin sentido.

### Resumen capitulo 2

La ingenier√≠a de indicaciones combina arte y ciencia para optimizar respuestas de modelos generativos. T√©cnicas clave: indicaciones claras, ejemplos consistentes, cadena de pensamientos, y configuraci√≥n apropiada de par√°metros (temperatura, top-k, top-p). Estas t√©cnicas no modifican pesos del modelo - para personalizaci√≥n profunda se requiere entrenamiento espec√≠fico con datos propios.


---


## 3. MODELOS DE LENGUAJE GRANDES B√ÅSICOS

### Introducci√≥n a la formaci√≥n previa
La formaci√≥n de un modelo multibillonario de par√°metros desde cero, llamada **formaci√≥n previa**, requiere millones de horas de c√°lculo de GPU, billones de componentes l√©xicos de datos y mucha paciencia. En este cap√≠tulo se explorar√° c√≥mo se forma un modelo b√°sico, incluyendo los objetivos de formaci√≥n y las leyes de escalamiento.

> üìå **Ejemplo:** BloombergGPT requiri√≥ una gran cantidad de GPU de 1.3 millones de horas, fue formado con un gran grupo distribuido de instancias de GPU utilizando Amazon SageMaker.

### Modelos de lenguaje grandes b√°sicos disponibles
Al comienzo de cualquier proyecto de IA generativa, primero debe explorar la gran cantidad de modelos b√°sicos formados previamente, disponibles p√∫blicamente, que existen hoy en d√≠a, incluyendo las variantes del modelo Llama 2 de Meta que se utilizan en este libro.

Muchos modelos generativos han sido formados con datos p√∫blicos de Internet, con muchos idiomas y temas diferentes. Estos modelos generan una comprensi√≥n s√≥lida del lenguaje humano, as√≠ como una cantidad de conocimientos en muchos dominios.

**Centros de modelos disponibles:**
- Hugging Face Model Hub
- PyTorch Hub  
- Amazon SageMaker JumpStart

**Datos de formaci√≥n de BloombergGPT:**

| Fuente | Tama√±o |
|--------|--------|
| **Datos financieros** | |
| Web | 42% |
| Noticias | 5% |
| Expedientes | 2% |
| Prensa | 1% |
| Bloomberg | 1% |
| TOTAL | 51% |
| **Otros datos (p√∫blicos)** | |
| C4 | 26% |
| Wikipedia | 20% |
| TOTAL | 49% |

### Analizadores l√©xicos
Cada modelo de IA generativa basado en lenguaje tiene un **analizador l√©xico** que convierte texto legible por seres humanos (por ejemplo, mensajes) en un vector que contiene identidades de analizadores l√©xicos (token_ids) o de las entradas (input_ids).

<div align="center">
    <img src="/imagenes/analizador-lexico.jpeg" width="70%">
</div>

Cada input_id representa un componente l√©xico en el vocabulario del modelo.

> üìå **NOTA:** input_ida en un mont√≥n de c√≥digo fuente de aplicaci√≥n de IA generativa, ya que estas son las representaciones num√©ricas de cada componente l√©xico. Una lista de estas son input_ids representa una pieza m√°s grande de texto, como una frase, oraci√≥n o p√°rrafo.

### Vectores de incrustaci√≥n
Los vectores de incrustaci√≥n, a menudo llamados "incrustaciones", se han utilizado en el aprendizaje autom√°tico, la recuperaci√≥n de informaci√≥n y los casos de uso de b√∫squeda durante d√©cadas. Las incrustaciones son una representaci√≥n num√©rica y vectorizada de cualquier entidad de cualquier tipo, incluyendo texto, im√°genes, videos y archivos de audio, proyectada en espacios vectoriales de dimensi√≥n muy alta.

<div align="center">
    <img src="/imagenes/vectores-incrustacion2.jpeg" width="70%">
</div>

<div align="center">
    <img src="/imagenes/vectores-incrustacion.jpeg" width="70%">
</div>

Para mayor simplicidad, usemos un espacio vectorial tridimensional simple en el cual cada incrustaci√≥n sea un vector de tres valores proyectados en el espacio tridimensional. Aqu√≠, puede ver que los componentes l√©xicos, como "ense√±ar" y "libro", est√°n estrechamente relacionados, mientras que otros, como "coche" y "fuego", est√°n m√°s lejos.

### Transformadores
Lanzados en 2017, los transformadores est√°n en el trono de la mayor√≠a de los modelos de lenguaje modernos. De hecho, la "T" en BERT y GPT, dos arquitecturas ling√º√≠sticas muy populares, significa transformador.

<div align="center">
    <img src="/imagenes/transformador-avanzado.jpeg" width="70%">
</div>

**Componentes principales del transformador:**

#### Capa de incrustaci√≥n
Se aprenden durante la preformaci√≥n del modelo y, en realidad, son parte del transformador m√°s grande. Cada componente l√©xico de entrada en la ventana contextual se asigna a una incrustaci√≥n.

#### Codificador
Usualmente, el codificador codifica ‚Äîo proyecta‚Äî secuencias de componentes l√©xicos de entrada en un espacio vectorial que representa la estructura y el significado de la oraci√≥n. La representaci√≥n de este espacio se aprende al preformar el modelo.

#### Autoservicio
Los transformadores utilizan un mecanismo llamado **autoatenci√≥n** para atender a los componentes l√©xicos de inter√©s mientras pasa por las entradas. La autoatenci√≥n se utiliza para asistir a cada componente l√©xico con la entrada de datos y todos los dem√°s componentes l√©xicos en la secuencia de entrada.

<div align="center">
    <img src="/imagenes/autoatencion.jpeg" width="70%">
</div>

La atenci√≥n por pares permite al modelo aprender las dependencias contextuales, o la comprensi√≥n contextual, entre todos los datos durante la formaci√≥n previa del modelo.

**Mecanismo de atenci√≥n:**
El transformador aprende en realidad conjuntos m√∫ltiples de pesos de autoservicio a trav√©s de la misma entrada y aprende aspectos diferentes del idioma. Cada cabeza funciona en paralelo sobre la misma entrada y aprende aspectos diferentes del idioma.

<div align="center">
    <img src="/imagenes/atencion-detalles.jpeg" width="70%">
</div>

<div align="center">
    <img src="/imagenes/atencion-detalles2.jpeg" width="70%">
</div>

#### Decodificador
Los pesos de atenci√≥n se pasan a trav√©s del resto de la red neuronal del transformador, independiente es el decodificador. El decodificador utiliza la comprensi√≥n contextual basada en la atenci√≥n de los componentes l√©xicos de entrada para ayudar a generar nuevos componentes l√©xicos que, en √∫ltima instancia, responden a la entrada proporcionada.

#### Salida softmax
La capa de salida softmax genera una distribuci√≥n de probabilidad en todo el vocabulario del componente l√©xico, en la que se asigna a cada componente cierta probabilidad de ser seleccionado enseguida.

<div align="center">
    <img src="/imagenes/salida-softmax.jpeg" width="70%">
</div>

### Tipos de modelos b√°sicos basados en transformadores
Hay tres variantes de modelos generativos basados en transformadores: **solo codificador**, **solo decodificador** y **codificador-decodificador**. Cada variante se forma con un objetivo de formaci√≥n diferente y durante la formaci√≥n previa se actualizan los pesos del modelo para minimizar la p√©rdida de los objetivos de formaci√≥n descritos a continuaci√≥n.

#### Modelos de solo codificador (autocodificadores)
Los modelos de solo codificador, o autocodificadores, se preforman mediante una t√©cnica denominada **modelado de lenguaje enmascarado (MLM)**, que enmascara aleatoriamente los componentes l√©xicos de entrada e intenta predecir cu√°les son enmascarados.

<div align="center">
    <img src="/imagenes/modelado-enmascarado.jpeg" width="70%">
</div>

**Objetivo:** Reconstruir el texto (eliminaci√≥n de ruido)
**Caracter√≠sticas:** Utilizan representaciones bidireccionales de la entrada para comprender mejor el contexto completo de un componente l√©xico.

#### Modelos de solo decodificador (autorregresivos)
Los modelos de solo decodificador, o modelos autorregresivos, se preforman utilizando un **modelo ling√º√≠stico causal unidireccional (CLM)**, que predice el componente l√©xico siguiente usando solo los componentes l√©xicos anteriores.

<div align="center">
    <img src="/imagenes/modelo-causal.jpeg" width="70%">
</div>

**Objetivo:** Predecir el siguiente componente l√©xico
**Caracter√≠sticas:** Solo revelan los componentes l√©xicos que conducen al que se predice.

**Modelos populares:** GPT-3 Falcon y LLaMA son modelos autorregresivos bien conocidos.

> üìå **NOTA:** Meta cambi√≥ el caso del nombre del modelo Llama cuando lanz√≥ Llama 2. La primera versi√≥n utiliza el caso mixto (LLaMA), que es un acr√≥nimo de Large Language Model Meta AI (Modelo de lenguaje grande para IA de Meta). La segunda versi√≥n utiliza el caso del t√≠tulo (Llama 2).

#### Modelos de codificador-decodificador (secuencia a secuencia)
Los modelos de secuencia a secuencia, originalmente dise√±ados para la traducci√≥n, tambi√©n son muy √∫tiles para tareas de resumen de texto. T5 y su hermano con ajuste fino, FLAN-T5, son modelos bien conocidos de codificador-decodificador, secuencia a secuencia, utilizados en un n√∫mero amplio de tareas de lenguaje generativo.

<div align="center">
    <img src="/imagenes/secuencia-secuencia.jpeg" width="70%">
</div>

**Objetivo:** Reconstrucci√≥n del tramo
**Caracter√≠sticas:** Utilizan tanto el transformador codificador como el decodificador.

### Conjuntos de datos para formaci√≥n previa
Un modelo generativo aprende las funciones durante la fase de formaci√≥n previa, cuando se ve una gran cantidad de datos de formaci√≥n, a menudo en la escala de terabytes y petabytes. Los conjuntos de datos a menudo provienen de Internet (datos p√∫blicos), pero tambi√©n pueden incluir datos privados de los dep√≥sitos o bases de datos privadas de Amazon S3.

**Conjuntos de datos populares:**
- **Wikipedia y Common Crawl**: Wikipedia ofrece un extracto multiling√ºe de su contenido de 2022, mientras que Common Crawl es un registro mensual de texto que se encuentra en todo Internet.
- **Colossal Clean Crawled Corpus (C4)**, **The Pile** y **RefinedWeb**: Intentan limpiar los datos para una formaci√≥n de modelos de mayor calidad.

> üìå **NOTA:** La familia de modelos Falcon fue formada con 1.5 billones de componentes l√©xicos de datos llamados RefinedWeb. Los datos se procesaron en un grupo de 257 instancias ml.c5.18xlarge de SageMaker, compuesto por 18 504 CPU y 37 TB de RAM de CPU.

### Leyes de escalamiento
Para los modelos generativos, ha surgido un conjunto de **leyes de escalamiento** que describen las compensaciones entre el tama√±o del modelo y el del conjunto de datos para un presupuesto inform√°tico fijo (por ejemplo, el n√∫mero de horas de GPU).

<div align="center">
    <img src="/imagenes/Requisitos-preformacion.jpeg" width="70%">
</div>

<div align="center">
    <img src="/imagenes/escalamiento-modelos.jpeg" width="70%">
</div>

Estas leyes de escalamiento establecen que puede lograr un mejor rendimiento del modelo generativo aumentando el n√∫mero de componentes l√©xicos o el n√∫mero de par√°metros del modelo.

### Modelos inform√°ticos √≥ptimos
En 2022, un grupo de investigadores public√≥ un art√≠culo que comparaba el rendimiento del modelo de varias combinaciones de modelos y tama√±os de conjuntos de datos. Dado que los autores nombraron el modelo final de informatizaci√≥n √≥ptima Chinchilla, este art√≠culo es conocido como el **art√≠culo Chinchilla**.

El art√≠culo Chinchilla implica que los modelos enormes, de par√°metros de m√°s de 100 mil millones, como el GPT-3, pueden estar sobreparametrizados y poco formados. Adem√°s, plantean la idea de que se podr√≠a lograr un rendimiento de m√°s de 100 mil millones de par√°metros con un modelo peque√±o simplemente proporcionando m√°s datos de formaci√≥n al modelo m√°s peque√±o.

**Leyes de escalamiento de Chinchilla:**

| Modelo | Tama√±o del modelo (par√°metros) | Tama√±o √≥ptimo del conjunto de datos (componentes l√©xicos) | Tama√±o real del conjunto de datos (componentes l√©xicos) | Hip√≥tesis |
|--------|-------------------------------|----------------------------------------------------------|-------------------------------------------------------|-----------|
| Chinchilla | 70 B | 1.4 T | 1.4 T | C√°lculo √≥ptimo (20√ó) |
| LLaMA-65B | 65 B | 1.3 T | 1.4 T | C√°lculo √≥ptimo (20√ó) |
| GPT-3 | 175 B | 3.5 T | 300 B | Sobreparametrizado para el tama√±o del conjunto de datos (~20√ó) |
| OPT-175B | 175 B | 3.5 T | 180 B | Sobreparametrizado para el tama√±o del conjunto de datos (~20√ó) |
| BLOOM | 176 B | 3.5 T | 350 B | Sobreparametrizado para el tama√±o del conjunto de datos (~20√ó) |
| Llama2-70B | 70 B | 1.4 T | 2.0 T | Mejor que el c√°lculo √≥ptimo (~20√ó) |

> üìå **NOTA:** Los modelos de m√°s de 175 mil millones deben ser formados con 3.5 billones de componentes l√©xicos. En cambio, fueron formados con 180-350 mil millones de componentes l√©xicos, un orden de magnitud menor de lo recomendado.

El modelo m√°s reciente de Llama 2 de 70 mil millones de par√°metros, que fue lanzado despu√©s del documento Chinchilla, fue formado con 2 trillones de componentes l√©xicos, mayor que la relaci√≥n de 20 componentes l√©xicos a 1 par√°metro descrita por el documento. Llama 2 super√≥ al modelo original de LLaMA basado en varios puntos de referencia, incluyendo la comprensi√≥n masiva del lenguaje multitarea (MMLU).

### Resumen
En este cap√≠tulo, se vio c√≥mo los modelos b√°sicos se forman usando cantidades grandes de texto durante la etapa inicial de formaci√≥n, llamada **formaci√≥n previa**. Aqu√≠ es donde el modelo desarrolla su comprensi√≥n del lenguaje.

Tambi√©n aprendimos tres tipos diferentes de modelos de lenguaje basados en transformadores: de **solo codificador** (autocodificaci√≥n), de **solo decodificador** (autorregresivo) y de **codificador-decodificador** (secuencia a secuencia).

Adem√°s, se exploraron las **leyes de escalamiento** que ayudan a los investigadores a elegir el n√∫mero de par√°metros del modelo y el tama√±o del conjunto de datos para un presupuesto inform√°tico determinado al preparar un modelo b√°sico desde cero.